{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.io as io\n",
    "import os\n",
    "\n",
    "# Define your device (for GPU processing)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),  # Resize frames to 128x128\n",
    "    transforms.ToTensor(),  # Convert frames to tensor format\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the RGB channels\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, max_frames=400, myframes = 0):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.max_frames = max_frames  # Maximum frames to load\n",
    "        self.myframes = myframes\n",
    "        self.classes = ['non_violent', 'violent']\n",
    "        self.data = []\n",
    "        \n",
    "        for label, class_name in enumerate(self.classes):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for cam_folder in os.listdir(class_dir):\n",
    "                cam_dir = os.path.join(class_dir, cam_folder)\n",
    "                if os.path.isdir(cam_dir):  \n",
    "                    for video_name in os.listdir(cam_dir):\n",
    "                        if video_name.endswith('.mp4'):\n",
    "                            video_path = os.path.join(cam_dir, video_name)\n",
    "                            self.data.append((video_path, label)) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path, label = self.data[idx]\n",
    "        \n",
    "        # Load the video and all its frames\n",
    "        video, _, _ = io.read_video(video_path, pts_unit='sec')\n",
    "\n",
    "        # Print original shape\n",
    "        # print(f\"Original video shape: {video.shape}\")\n",
    "        self.myframes = self.myframes + video.shape[0]\n",
    "        # Number of frames in the video\n",
    "        total_frames = video.shape[0]\n",
    "\n",
    "        # Initialize the padded tensor\n",
    "        if total_frames < self.max_frames:\n",
    "            # Create a new tensor filled with zeros\n",
    "            padded_video = torch.zeros((self.max_frames, video.shape[1], video.shape[2], video.shape[3]), dtype=video.dtype)\n",
    "            # Copy the existing frames into the padded tensor\n",
    "            padded_video[:total_frames] = video  # Fill the start with actual frames\n",
    "            video = padded_video  # Use the padded video\n",
    "        else:\n",
    "            # If we have enough frames, just take the first max_frames\n",
    "            video = video[:self.max_frames]\n",
    "\n",
    "        # Print shape after padding/truncating\n",
    "        # print(f\"Processed video shape: {video.shape}\")\n",
    "\n",
    "        # Apply transformations to each frame\n",
    "        pil_transform = transforms.ToPILImage()\n",
    "        frames = []\n",
    "        for frame in video:\n",
    "            # Convert to [channels, height, width]\n",
    "            frame = frame.permute(2, 0, 1)  # Swap dimensions\n",
    "\n",
    "            pil_img = pil_transform(frame)  # Convert to PIL\n",
    "            if self.transform:\n",
    "                pil_img = self.transform(pil_img)  # Apply transformation\n",
    "            frames.append(pil_img)\n",
    "        \n",
    "        video = torch.stack(frames)  # Stack the frames back into a tensor\n",
    "        video = video.to(device)\n",
    "        label = torch.tensor(label).to(device)\n",
    "        print(self.myframes)\n",
    "        return video, label\n",
    "    def avg_frames(self):\n",
    "        return self.myframes / len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original video shape: torch.Size([164, 1080, 1920, 3])\n",
      "Processed video shape: torch.Size([400, 1080, 1920, 3])\n",
      "Original video shape: torch.Size([239, 1080, 1920, 3])\n",
      "Processed video shape: torch.Size([400, 1080, 1920, 3])\n",
      "Original video shape: torch.Size([111, 1080, 1920, 3])\n",
      "Processed video shape: torch.Size([400, 1080, 1920, 3])\n",
      "Original video shape: torch.Size([215, 1080, 1920, 3])\n",
      "Processed video shape: torch.Size([400, 1080, 1920, 3])\n",
      "Original video shape: torch.Size([290, 1080, 1920, 3])\n",
      "Processed video shape: torch.Size([400, 1080, 1920, 3])\n"
     ]
    }
   ],
   "source": [
    "path ='/home/subru/projects/CNN/A-Dataset-for-Automatic-Violence-Detection-in-Videos-master/violence-detection-dataset'\n",
    "train_dataset = VideoDataset(root_dir=path, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "# Iterate through the data loader\n",
    "for videos, labels in train_loader:\n",
    "    print(videos.shape)  # Should print shape [batch_size, frames, channels, height, width]\n",
    "    print(labels)        # Should print corresponding labels (0 for non-violent, 1 for violent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
